{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from keras.preprocessing import text,sequence\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import wordnet\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Dropout,Embedding\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as pyplot\n",
    "import seaborn\n",
    "import nltk\n",
    "import gensim\n",
    "import pickle\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test(text):\n",
    "    data_small = pd.DataFrame(np.array([text]),columns=['text'])\n",
    "\n",
    "    special_characters = '!?@#$%^&*()-+_=,<>/'\n",
    "\n",
    "    data_small['text_character_cnt'] = data_small['text'].str.len()\n",
    "    # if(data['text_character_cnt'] > 0):\n",
    "    data_small['text_word_cnt'] = data_small['text'].str.split().str.len()\n",
    "    # data['text_character_per_word'] = data['text_character_cnt'] / data['text_word_cnt']\n",
    "\n",
    "    data_small['text_special_cnt'] = data_small['text'].apply(lambda x: len([x for x in x.split() if any(char in special_characters for char in x)]))\n",
    "\n",
    "    for char in special_characters:\n",
    "        data_small['text_' + char + '_per_char'] = data_small['text'].apply(lambda x: len([x for x in x.split() if char in x]))\n",
    "        data_small['text_' + char + '_per_word'] = data_small['text'].apply(lambda x: len([x for x in x.split() if char in x]))\n",
    "\n",
    "    data_small['text_http_cnt'] = data_small['text'].apply(lambda x: len([x for x in x.split() if 'http' in x]))\n",
    "    data_small['text_www_cnt'] = data_small['text'].apply(lambda x: len([x for x in x.split() if 'www' in x]))\n",
    "    data_small['text_number_cnt'] = data_small['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "    \n",
    "    return data_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_html(text):\n",
    "    soup=BeautifulSoup(text,\"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]','',text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    return re.sub(r'http\\S+','',text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    final_text=[]\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop_words:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "def remove_HashOrAT(text):\n",
    "    return re.sub('@|#','',text)\n",
    "\n",
    "def remove_puncuation(text):\n",
    "    return text.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "def remove_uppercase(text):\n",
    "    return text.lower()\n",
    "    \n",
    "\n",
    "def clean_text_data(text):\n",
    "    text=string_html(text)\n",
    "    text=remove_square_brackets(text)\n",
    "    text=remove_URL(text)\n",
    "    text=remove_HashOrAT(text)\n",
    "    text=remove_puncuation(text)\n",
    "    text=remove_uppercase(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model1_News:\n",
    "    def __init__(self):\n",
    "        self.model = pickle.load(open(\"decisionTreeModel.pkl\",'rb'))\n",
    "    \n",
    "    def preditWithText(self, text):\n",
    "        return int(self.model.predict(prepare_test(text).drop(columns=['text']))[0])\n",
    "    \n",
    "    def return_str(self, text):\n",
    "        if self.preditWithText(text) == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "class model1_tweets:\n",
    "    def __init__(self):\n",
    "        self.model = pickle.load(open(\"twitterTFModel.pkl\",'rb'))\n",
    "        self.key = realorfake = {\"True\":1,\"False\":0}\n",
    "        \n",
    "    \n",
    "    def preditWithText(self, text):\n",
    "        return self.model.predict([text])[0]\n",
    "\n",
    "    def return_str(self, text):\n",
    "        if self.preditWithText(text) == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "model1 = model1_tweets()\n",
    "model2 = model1_News()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class joinmodel:\n",
    "    def __init__(self):\n",
    "        self.model1 = model1_tweets()\n",
    "        self.model2 = model1_News()\n",
    "        self.lastResult = None\n",
    "    def getTrueFalse(self, text, verbose = False):\n",
    "        result = {}\n",
    "        if type(text) != str:\n",
    "            return \"Error please feed in a string\"\n",
    "        \n",
    "        result[\"modelTweets_1\"] = self.getTweetModel_1(text)\n",
    "        result[\"modelNews_2\"] = self.getNewsModel_2(text)\n",
    "        result[\"combinedResult\"] = self.getCombinedResults(result.get(\"modelTweets_1\"), result.get(\"modelNews_2\")) \n",
    "        self.lastResult = result\n",
    "        \n",
    "        if verbose:\n",
    "            return result\n",
    "        else:\n",
    "            return result.get(\"combinedResult\")\n",
    "        \n",
    "    \n",
    "    def getTweetModel_1(self, text):\n",
    "        return self.model1.return_str(text)\n",
    "    def getNewsModel_2(self, text):\n",
    "        return self.model2.return_str(text)\n",
    "    \n",
    "    # this will be replaced with a lasso regression model trained with both models but this is a simple implemtation\n",
    "    # if both results are the same then it returns that boolean. otherwise it returns False\n",
    "    def getCombinedResults(self, model1_result, model2_result):\n",
    "        if model1_result == model2_result:\n",
    "            return model2_result\n",
    "        else: \n",
    "            return False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = joinmodel()\n",
    "test.getTrueFalse(\"Donald Trump is President\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelTweets_1': False, 'modelNews_2': True, 'combinedResult': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.getTrueFalse(\"Donald Trump is President\", verbose=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "654dd77d1753ac1e3fb7ea93f4452340a94bf76edae2b51bda429fccd972a95a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
