{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.DataFrame()\n",
    "realorfake = {\"real\":1,\"fake\":0}\n",
    "\n",
    "for dir in os.listdir('../data/politifact'):\n",
    "    for datadir in os.listdir('../data/politifact/' + dir):\n",
    "        try:\n",
    "            with open('../data/politifact/' + dir + '/' + datadir + '/news content.json') as json_file:\n",
    "                json_obj = json.load(json_file)\n",
    "                json_obj['real'] = realorfake.get(dir)\n",
    "                data = data.append(json_obj, ignore_index=True)\n",
    "        except Exception:\n",
    "            None\n",
    "        finally:\n",
    "            None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_characters = '!?@#$%^&*()-+_=,<>/'\n",
    "\n",
    "data['text_character_cnt'] = data['text'].str.len()\n",
    "data['text_word_cnt'] = data['text'].str.split().str.len()\n",
    "data['text_character_per_word'] = data['text_character_cnt'].combine(data['text_word_cnt'], lambda x, y: x / y if y else 0)\n",
    "\n",
    "data['text_special_cnt'] = data['text'].apply(lambda x: len([x for x in x.split() if any(char in special_characters for char in x)]))\n",
    "data['text_special_per_char'] = data['text'].combine(data['text_character_cnt'], lambda x, y: (len([x for x in x.split() if any(char in special_characters for char in x)]) / y) if y else 0)\n",
    "data['text_special_per_word'] = data['text'].combine(data['text_word_cnt'], lambda x, y: (len([x for x in x.split() if any(char in special_characters for char in x)]) / y) if y else 0)\n",
    "\n",
    "for char in special_characters:\n",
    "    data['text_' + char + '_cnt'] = data['text'].apply(lambda x: len([x for x in x.split() if char in x]))\n",
    "    data['text_' + char + '_per_char'] = data['text'].combine(data['text_character_cnt'], lambda x, y: (len([x for x in x.split() if char in x]) / y) if y else 0)\n",
    "    data['text_' + char + '_per_word'] = data['text'].combine(data['text_word_cnt'], lambda x, y: (len([x for x in x.split() if char in x]) / y if y else 0))\n",
    "\n",
    "data['text_http_cnt'] = data['text'].apply(lambda x: len([x for x in x.split() if 'http' in x]))\n",
    "data['text_www_cnt'] = data['text'].apply(lambda x: len([x for x in x.split() if 'www' in x]))\n",
    "data['text_number_cnt'] = data['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "\n",
    "data['title_character_per_word'] = data['title_character_cnt'].combine(data['title_word_cnt'], lambda x, y: x / y if y else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['autor_cnt'] = data['authors'].apply(lambda x: len(x))\n",
    "\n",
    "authors_dict = gensim.corpora.Dictionary(data['authors'])\n",
    "\n",
    "for author in authors_dict:\n",
    "    data[authors_dict[author]] = data['authors'].apply(lambda x: 1 if authors_dict[author] in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dict = gensim.corpora.Dictionary([data['source'].unique()])\n",
    "data['source_id'] = data['source'].apply(lambda x: list(source_dict.values()).index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['url', 'text', 'images', 'top_img', 'keywords', 'authors',\n",
    "       'canonical_link', 'title', 'meta_data', 'movies', 'publish_date',\n",
    "       'source', 'summary', 'real'])\n",
    "Y = data['real']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "score = accuracy_score(Y_test, prediction)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['character_cnt'] = data['title'].str.len()\n",
    "data['word_cnt'] = data['title'].str.split().str.len()\n",
    "data['character_per_word'] = data['character_cnt'] / data['word_cnt']\n",
    "\n",
    "special_characters = '!?@#$%^&*()-+_=,<>/'\n",
    "data['special_cnt'] = data['title'].apply(lambda x: len([x for x in x.split() if any(char in special_characters for char in x)]))\n",
    "\n",
    "data['hashtag_cnt'] = data['title'].apply(lambda x: len([x for x in x.split() if '#' in x]))\n",
    "data['at_cnt'] = data['title'].apply(lambda x: len([x for x in x.split() if '@' in x]))\n",
    "data['explanation_cnt'] = data['title'].apply(lambda x: len([x for x in x.split() if '!' in x]))\n",
    "data['question_cnt'] = data['title'].apply(lambda x: len([x for x in x.split() if '?' in x]))\n",
    "data['interrobang_cnt'] = data['title'].apply(lambda x: len([x for x in x.split() if '?!' in x]))\n",
    "data['ellipsis_cnt'] = data['title'].apply(lambda x: len([x for datax in x.split() if '...' in x]))\n",
    "data['semicolon_cnt'] = data['title'].apply(lambda x: len([x for x in x.split() if ';' in x]))\n",
    "\n",
    "data['http_cnt'] = data['title'].apply(lambda x: len([x for x in x.split() if 'http' in x]))\n",
    "data['www_cnt'] = data['title'].apply(lambda x: len([x for x in x.split() if 'www' in x]))\n",
    "\n",
    "data['number_cnt'] = data['title'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data['title_words'] = data['title'].apply(lambda x: word_tokenize(x))\n",
    "data['title_sentences'] = data['title'].apply(lambda x: sent_tokenize(x))\n",
    "title_word_dict = gensim.corpora.Dictionary(data['title_words'])\n",
    "title_sentence_dict = gensim.corpora.Dictionary(data['title_sentences'])\n",
    "data['title_words'] = data['title_words'].apply(lambda x: [title_word_dict.doc2bow(x) for word in x])\n",
    "data['title_sentences'] = data['title_sentences'].apply(lambda x: [title_sentence_dict.doc2bow(x) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['title', 'index', 'id', 'news_url', 'tweet_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['real'])\n",
    "Y = data['real']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "score = accuracy_score(Y_test, prediction)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1375c670ee5f5aa255916eacc89fe4f0e38c25cf8703a8220a4512e9d04ef996"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
